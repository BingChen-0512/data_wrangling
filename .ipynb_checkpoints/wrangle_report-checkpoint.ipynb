{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering\n",
    "I started this project from gathering data. The following are the steps of gathering all the three tables:\n",
    "<ul>\n",
    "    <li>I downloaded 'twitter-archive-enhanced.csv' from Udacity website, and then loaded it into my workspace by 'pandas.read_csv'.</li>\n",
    "       <li> 'image_predictions.tsv' file was downloaded by requests.get(url). I saved it on my PC by 'file.write', and then imported it into the workspace by 'pandas.read_csv' too.</li>\n",
    "        <li>I created twiiter account and developer account first. After that, I utilized the hints on the project description and answers on Udacity knowledge platform to scrape data from API. I knew that timestamp data was not required to get from API, but I still get it out of curiosity. I also put the answers that helped me as the references at the end of my Jupyter notebook.</li></ul>\n",
    "The names of the tables were 'twitter_archive', 'image_predictions', and 'Data_from_API'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing\n",
    "<p>To visually assess the 'twitter-archive-enhanced' table, Excel came to help in examining the weird data about rating numbers. I documented the 'tweet_id' and the correct ratings for those tweets. The quality issue that 'text' column contained both texts and urls of the tweets was also found in Excel.</p>\n",
    "<p>I also used some urls in 'text' column to make sure that some tweets should be deleted, as they were not relevant to dogs or had no ratings.</p>\n",
    "<p>As for programmatic assess, the major methods I used are:</p>\n",
    "<ul><li>DataFrame.shape</li>\n",
    "    <li>DataFrame.info( )</li>\n",
    "    <li>DataFrame.describe( )</li>\n",
    "    <li>DataFrame.sample(n)</li>\n",
    "    <li>DataFrame.head( )</li>\n",
    "    <li>Series.value_counts( )</li>\n",
    "    <li>slicing methods of dataframe</li></ul>\n",
    "<p> Some data quality issues were identified during my cleaning procedure. They are:</p>\n",
    "<ul><li> the tweet of id 810984652412424192, has no rating</li>\n",
    "<li> the column name 'id' in 'Data_from_API' table is inconsistent with the other tables ('tweet_id')</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "<p>The first step of cleaning data was to make copies of all the tables. The tables for cleaning were 'twitter_archive_clean', 'image_predictions_clean', and 'Data_from_API_clean'. I splited the cleaning process into 3 parts: before merging, merging, and after merging.</p>\n",
    "<p>Before merging tables, I filtered out the replies and retweets in 'twitter_archive_clean' table and combined the columns about dog stages. Plus, column names of tweet ids were adjusted to be consistent for the convenience of merging. The data types of tweet-ids in 3 tables were changed to be string.</p>  \n",
    "<p>After that, I merged 'twitter_archive_clean' table with 'Data_from_API_clean' table by innner join to remove the tweets that couldn't be found on Twitter API. Next, I merged the new table with 'image_predictions_clean' table. The 'image_prediction_clean' table was to help filter out the tweets that were not dogs. Inner join method was used again to ensure that the tweets in the final table were all about dogs. The final table was named 'twitter_archive_master' and copied again.</p>\n",
    "<p>After merging, I droped 'timestamp_y' column and renamed 'timestamp_x' to be 'timestamp'. The data type of 'timestamp' column was changed to be datetime. The tweets not about dogs were removed by 'twitter_archive_master_clean.query('p1_dog==True')'. </p>\n",
    "    <p>Next, I updated the inaccurate ratings by iterating them. Tweets about plagiarism and without ratings were deleted. Thanks to the testing result, I was reminded to reset the index after deleting many rows. Then I tried to dealed with wrongly extracted names. First, I replaced the dog names start with lower case letters and 'None' with NaNs. In Excel, I found that the text patern 'her/his name is XXX' was not recognized. I made up regex to extract names from that patern. However, the result came out that there were no tweets with such text patern. I guessed it was due to the filtering actions, those tweets were already excluded. Still, I was glad to be more familiar with regex. With regex, I successfully splited urls from the original 'text' column. </p>\n",
    "<p>After testing, I finished the wrangling efforts by storing the data to a CSV file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
